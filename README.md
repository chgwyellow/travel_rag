<p align="center">
  <img src="docs/banner.svg" width="100%" />
</p>

# ğŸŒ Travel RAG Assistant

ğŸ“š Retrieval-Augmented Generation System for Tourism Information

LangChain â€¢ Gemini 2.5 Flash â€¢ ChromaDB â€¢ Pinecone â€¢ Streamlit

## ğŸŒ Live Demo

> Deployment in progress. Check back soon for the live application!

<p align="center">
  <!-- Environment / Tooling -->
  <img src="https://img.shields.io/badge/Python-3.13-blue?logo=python&logoColor=white" />
  <img src="https://img.shields.io/badge/Poetry-1.8+-6A5ACD?logo=poetry" />
  <img src="https://img.shields.io/badge/LangChain-Latest-00A67E" />
  <img src="https://img.shields.io/badge/Gemini-Pro-4285F4?logo=google&logoColor=white" />
  <img src="https://img.shields.io/badge/ChromaDB-Supported-FF6F00" />
  <img src="https://img.shields.io/badge/Pinecone-Cloud-00C9A7" />
  <img src="https://img.shields.io/badge/Streamlit-Cloud-FF4B4B?logo=streamlit&logoColor=white" />
  <img src="https://img.shields.io/badge/License-MIT-green" />
</p>

---

# ğŸ“‘ Table of Contents

- [Overview](#overview)
- [Current Dataset](#-current-dataset)
- [Tech Stack](#-tech-stack)
- [Project Layout](#-project-layout)
- [Quick Start](#-quick-start)
- [Notebook / Chapter Overview](#-notebook--chapter-overview)
- [RAG Pipeline Architecture](#-rag-pipeline-architecture)
- [Future Work](#-future-work)
- [License](#-license)

---

## Overview

This project builds a complete Retrieval-Augmented Generation (RAG) system for tourism information using modern LLM technologies.

**Current Status:** Vector database setup complete (Chapters 01-03 completed)

**Goal:** Create an AI-powered travel assistant that can answer questions about Seattle tourism by retrieving relevant information from a vector database and generating natural language responses using Google Gemini.

**Key Components:**

- Geoapify API for attraction data
- Wikipedia API for detailed descriptions
- Vector database for semantic search
- LangChain for RAG orchestration  
- Streamlit for interactive web interface

## ğŸ“Š Current Dataset

- **Cities**: 13 major US cities
  - Seattle, New York, Washington DC, Chicago, San Francisco
  - Boston, Portland, Austin, Denver, Miami
  - Nashville, New Orleans, Las Vegas
- **Primary Source**: [Geoapify Places API](https://www.geoapify.com/)
- **Secondary Source**: [Wikipedia API](https://www.mediawiki.org/wiki/API:Main_page)
- **Total Records**: ~1,000+ attractions with Wikipedia descriptions
- **Largest Dataset**: Washington DC (264 attractions)
- **Format**: JSON (enriched with location data and descriptions)
- **Fields**: Name, Description, Location (lat/lon), Address, Categories, Place ID, City, State, Country

## ğŸ› ï¸ Tech Stack

### **Core Components**

| Component | Technology | Purpose |
|-----------|-----------|---------|
| **LLM** | Gemini 2.5 Flash | Text generation and question answering |
| **Embedding** | sentence-transformers (all-MiniLM-L6-v2) | Document vectorization (384 dimensions) |
| **Vector DB (Dev)** | ChromaDB | Local vector storage and retrieval |
| **Vector DB (Prod)** | Pinecone | Cloud-based vector database |
| **RAG Framework** | LangChain | Pipeline orchestration |
| **Frontend** | Streamlit | Interactive web interface |
| **Data Processing** | Pandas | Data manipulation and cleaning |

### **Development Tools**

- **Python**: 3.13
- **Package Manager**: Poetry
- **Environment**: python-dotenv
- **Logging**: Custom emoji logger

---

## ğŸ“ Project Layout

```text
.
â”œâ”€ data/
â”‚  â”œâ”€ raw/                      # Raw API responses
â”‚  â”‚  â””â”€ Seattle_attractions_raw.json
â”‚  â””â”€ processed/                # Processed & enriched data
â”‚     â”œâ”€ seattle_attractions_with_wikipedia.json
â”‚     â”œâ”€ seattle_attractions_enriched_with_location.json
â”‚     â”œâ”€ seattle_attractions_documents.json
â”‚     â””â”€ metadata.json
â”‚
â”œâ”€ chroma_db/                   # ChromaDB vector storage (local)
â”‚
â”œâ”€ notebook/                    # Jupyter Notebooks (exploration)
â”‚  â”œâ”€ 01_data_exploration.ipynb      # Geoapify API & data collection
â”‚  â””â”€ 02_data_enrichment.ipynb       # Wikipedia descriptions & location data
â”‚
â”œâ”€ src/
â”‚  â”œâ”€ app/                      # Streamlit web application
â”‚  â”‚  â””â”€ app.py
â”‚  â”œâ”€ data_collection/          # Data pipeline modules
â”‚  â”‚  â”œâ”€ geoapify_client.py    # Geoapify API client
â”‚  â”‚  â”œâ”€ wikipedia_client.py   # Wikipedia API client
â”‚  â”‚  â”œâ”€ collector.py          # Chapter 1 workflow
â”‚  â”‚  â”œâ”€ enricher.py           # Chapter 2 workflow
â”‚  â”‚  â””â”€ document_builder.py   # RAG document formatting
â”‚  â”œâ”€ rag/                      # RAG pipeline implementation
â”‚  â”‚  â””â”€ pipeline.py
â”‚  â”œâ”€ utils/                    # Utilities
â”‚  â”‚  â”œâ”€ logger.py
â”‚  â”‚  â””â”€ emoji_log.py          # Emoji-enhanced logging
â”‚  â””â”€ config.py                 # Configuration management
â”‚
â”œâ”€ scripts/                     # Utility scripts
â”‚  â”œâ”€ setup_chromadb.py
â”‚  â”œâ”€ ingest_data.py
â”‚  â””â”€ test_rag.py
â”‚
â”œâ”€ .env.example                 # Environment variables template
â”œâ”€ .gitignore
â”œâ”€ pyproject.toml
â”œâ”€ poetry.lock
â””â”€ README.md
```

---

## ğŸš€ Quick Start

### **1. Clone Repository**

```bash
git clone https://github.com/chgwyellow/travel_rag.git
cd travel_rag
```

### **2. Install Dependencies**

```bash
# Install Poetry (if not installed)
curl -sSL https://install.python-poetry.org | python3 -

# Install project dependencies
poetry install
```

### **3. Set Up Environment Variables**

```bash
# Copy example env file
cp .env.example .env

# Edit .env and add your API keys
GEOAPIFY_API_KEY=your_geoapify_api_key_here
EMAIL=your_email@example.com  # For Wikipedia API User-Agent
GOOGLE_API_KEY=your_gemini_api_key_here  # For future RAG pipeline
```

### **4. Run Data Collection Pipeline**

Execute the automated data pipeline to collect and enrich Seattle attractions:

```bash
# Run the complete pipeline (Chapters 1-2)
poetry run python scripts/ingest_data.py
```

This will:

- Fetch attractions from Geoapify API
- Filter attractions with Wikipedia links
- Enrich with Wikipedia descriptions
- Create RAG-ready documents

**Output files** (in `data/`):

- `raw/Seattle_attractions_raw.json` - Raw Geoapify data
- `processed/Seattle_attractions_with_wikipedia.json` - Filtered attractions
- `processed/Seattle_attractions_enriched.json` - Enriched with descriptions
- `processed/Seattle_attractions_documents.json` - RAG-ready documents

### **5. Build Vector Database**

After collecting and enriching the data, build the ChromaDB vector database:

```bash
# Build vector database for Seattle (default)
poetry run python scripts/setup_chromadb.py

# Or specify a different city
poetry run python scripts/setup_chromadb.py --city "New York"

# Custom collection name
poetry run python scripts/setup_chromadb.py --city Seattle --collection my_attractions
```

This will:

- Load processed documents from `data/processed/`
- Create HuggingFace embedding model (all-MiniLM-L6-v2)
- Initialize ChromaDB persistent client
- Generate 384-dimensional embeddings for all documents
- Store documents with metadata in ChromaDB

**Output:**

- `chroma_db/` directory with vector database
- Ready for semantic search and RAG pipeline

### **6. Explore with Jupyter Notebooks (Optional)**

```bash
# Start Jupyter to explore the data collection process
poetry run jupyter lab

# Open notebooks:
# - 01_data_exploration.ipynb (Geoapify API exploration)
# - 02_data_enrichment.ipynb (Wikipedia enrichment process)
```

### **6. Run Streamlit App (Coming Soon)**

After completing RAG pipeline implementation:

```bash
poetry run streamlit run src/app/app.py
```

---

## ğŸ““ Notebook / Chapter Overview

<details>
<summary><b>ğŸ“Š Chapter 01 â€” Data Exploration (Geoapify API)</b></summary>

ğŸ““ `01_data_exploration.ipynb`

**Objectives:**

- Set up Geoapify API for Seattle attractions
- Fetch tourism data within Seattle bounding box
- Filter attractions with Wikipedia links
- Analyze data structure and quality
- Design document format for RAG

**Implementation:**

- Geoapify Places API with `tourism` category filter
- Bounding box: Seattle metropolitan area
- Filtered for attractions with Wikipedia data
- Saved raw data to `data/raw/Seattle_attractions_raw.json`

**Key Findings:**

- 62 attractions with Wikipedia links (from ~500 total)
- All attractions have place_id, name, and location data
- Wikipedia codes in format "language:title" (e.g., "en:Space Needle")
- Categories include landmarks, museums, parks, monuments

**Output:**

- `seattle_attractions_with_wikipedia.json` - 62 filtered attractions
- Document format design for RAG
- Ready for Wikipedia enrichment

</details>

---

<details>
<summary><b>âœ¨ Chapter 02 â€” Data Enrichment (Wikipedia API)</b></summary>

ğŸ““ `02_data_enrichment.ipynb`

**Objectives:**

- Fetch Wikipedia descriptions for all 62 attractions
- Merge location data from raw Geoapify response
- Perform data quality analysis and cleaning
- Create RAG-ready document format
- Validate data completeness

**Implementation:**

- Wikipedia API with User-Agent header
- Batch fetching with 0.5s rate limiting
- Location data merge (lat, lon, address, city, state, postcode)
- Document format: Name + Location + Coordinates + Description

**Data Quality Results:**

- âœ… 62/62 attractions with descriptions (100% success)
- âœ… 0 duplicates (based on place_id)
- âœ… 27.4% descriptions contain special characters (normal)
- âœ… Average description length: 860 characters
- âœ… 100% data completeness

**Output:**

- `seattle_attractions_enriched_with_location.json` - Full enriched data
- `seattle_attractions_documents.json` - RAG-ready documents
- `metadata.json` - Updated with enrichment statistics
- Ready for vector database ingestion

</details>

---

<details>
<summary><b>ğŸ” Chapter 03 â€” Vector Database Setup (ChromaDB)</b></summary>

ğŸ““ `03_vector_database.ipynb`

**Objectives:**

- Set up ChromaDB and embedding models
- Generate embeddings for all attraction documents
- Implement semantic search functionality
- Test metadata filtering capabilities
- Validate retrieval quality

**Implementation:**

- HuggingFace embedding model: `sentence-transformers/all-MiniLM-L6-v2`
- ChromaDB PersistentClient for vector storage
- 384-dimensional embeddings for all documents
- Semantic similarity search with score analysis

**Quality Results:**

- âœ… 62/62 documents indexed successfully
- âœ… 100% test pass rate in quality validation
- âœ… Semantic search working correctly
- âœ… Average query time: <100ms
- âœ… Database size: ~50 MB

**Output:**

- `chroma_db/` - Persistent vector database storage
- `chroma_db/travel_attractions/` - Collection with embeddings
- Ready for RAG pipeline integration with LLM

</details>

---

## ğŸ§© RAG Pipeline Architecture

```mermaid
flowchart TD

%% ============================
%% COLOR THEMES
%% ============================
classDef data fill:#BBDEFB,stroke:#0D47A1,stroke-width:2px,color:#0D47A1,rx:10,ry:10
classDef process fill:#DCEDC8,stroke:#33691E,stroke-width:2px,color:#1B5E20,rx:10,ry:10
classDef rag fill:#FFE0B2,stroke:#E65100,stroke-width:2px,color:#E65100,rx:10,ry:10
classDef llm fill:#F8BBD0,stroke:#AD1457,stroke-width:2px,color:#880E4F,rx:10,ry:10
classDef ui fill:#D1C4E9,stroke:#4527A0,stroke-width:2px,color:#311B92,rx:10,ry:10

%% ============================
%% NODES
%% ============================

A["ğŸ“Š Raw Tourism Data<br>JSON from Open Data"]

subgraph DATA[Data Processing]
    B1["ğŸ§¹ Data Cleaning"]
    B2["ğŸ“ Document Creation<br>Merge Fields"]
    B3["âœ‚ï¸ Text Chunking<br>500 tokens, overlap 50"]
end

subgraph EMBED[Embedding & Storage]
    C1["ğŸ¤– SentenceTransformer<br>all-MiniLM-L6-v2<br>384-dim Vectors"]
    C2["ğŸ’¾ Vector Database<br>ChromaDB / Pinecone"]
end

D["ğŸ‘¤ User Query<br>Natural Language Question"]

subgraph RAG[RAG Pipeline]
    E1["ğŸ” Query Embedding<br>SentenceTransformer"]
    E2["ğŸ¯ Similarity Search<br>Top-K Retrieval"]
    E3["ğŸ“„ Context Assembly<br>Retrieved Documents"]
end

subgraph LLM[LLM Generation]
    F1["ğŸ’¬ Prompt Construction<br>Context + Query"]
    F2["ğŸ§  Gemini 2.5 Flash<br>Response Generation"]
end

G["âœ¨ AI Response<br>+ Source Citations"]

subgraph UI[User Interface]
    H1["ğŸ¨ Streamlit App<br>Interactive UI"]
    H2["ğŸ’¬ Conversation History"]
end

%% ============================
%% FLOWS
%% ============================

A --> B1 --> B2 --> B3 --> C1 --> C2

D --> E1 --> E2
C2 -.->|Vector Search| E2
E2 --> E3 --> F1 --> F2 --> G --> H1 --> H2

%% ============================
%% CLASS ASSIGNMENTS
%% ============================

class A,B1,B2,B3 data
class C1,C2 process
class E1,E2,E3 rag
class F1,F2 llm
class G,H1,H2 ui
```

---

## ğŸ”® Future Work

- [ ] **Additional Data Sources** - Restaurants, hotels, activities
- [ ] **Conversation History** - Multi-turn dialogue support
- [ ] **Map Integration** - Interactive geo-spatial visualization
- [ ] **Response Evaluation** - Quality metrics and user feedback
- [ ] **API Deployment** - RESTful API for integration

---

## ğŸ“œ License

MIT License (free to use & modify)

---

<p align="center">
  <i>Built with â¤ï¸ using LangChain and Google Gemini Pro</i>
</p>
