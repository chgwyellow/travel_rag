{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95d95af7",
   "metadata": {},
   "source": [
    "# ğŸ“– Chapter 02 â€” Data Processing & Chunking\n",
    "\n",
    "## ğŸ¯ Objectives\n",
    "\n",
    "In this chapter, we will process the raw tourism data into a format suitable for RAG implementation.\n",
    "\n",
    "We will create unified document structures, implement chunking strategies, and save processed data for vector database ingestion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1fae3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from src.config import RAW_DATA_DIR, PROCESSED_DATA_DIR\n",
    "import json\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from src.utils.emoji_log import step, task, data, info, success, save, done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c847f36d",
   "metadata": {},
   "source": [
    "## ğŸ§¹ Step 01 â€” Data Loading\n",
    "\n",
    "Load the raw tourism data from JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57c990c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(RAW_DATA_DIR / \"scenic_spot.json\", \"r\", encoding=\"utf-8-sig\") as f:\n",
    "    loaded_data = json.load(f)\n",
    "\n",
    "attraction = loaded_data[\"XML_Head\"][\"Infos\"][\"Info\"]\n",
    "df = pd.DataFrame(attraction)\n",
    "\n",
    "text_cols = [\"Add\", \"Description\", \"Opentime\", \"Ticketinfo\", \"Travellinginfo\"]\n",
    "df[text_cols] = df[text_cols].fillna(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9dd86a5",
   "metadata": {},
   "source": [
    "## ğŸ“„ Step 02 â€” Document Creation\n",
    "\n",
    "Transform each attraction into a unified document format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "705dcaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_document(row):\n",
    "    # Combine the text columns\n",
    "    content_parts = [\n",
    "        f\"Name: {row['Name']}\",\n",
    "        f\"Region: {row['Region']}\",\n",
    "    ]\n",
    "\n",
    "    # if column exists, return it, otherwise, return \"\", and \"\" is false in if\n",
    "    if str(row.get(\"Add\", \"\")).strip():\n",
    "        content_parts.append(f\"Address: {row['Add']}\")\n",
    "\n",
    "    if str(row.get(\"Description\", \"\")).strip():\n",
    "        content_parts.append(f\"Description: {row['Description']}\")\n",
    "\n",
    "    if str(row.get(\"Opentime\", \"\")).strip():\n",
    "        content_parts.append(f\"Open_time: {row['Opentime']}\")\n",
    "\n",
    "    if str(row.get(\"Ticketinfo\", \"\")).strip():\n",
    "        content_parts.append(f\"Ticket: {row['Ticketinfo']}\")\n",
    "\n",
    "    # Filter the empty\n",
    "    content = \"\\n\".join([part for part in content_parts if part])\n",
    "\n",
    "    return {\n",
    "        \"id\": row[\"Id\"],\n",
    "        \"content\": content,\n",
    "        \"metadata\": {\n",
    "            \"name\": row[\"Name\"],\n",
    "            \"region\": row[\"Region\"],\n",
    "            \"category\": row.get(\"Class1\", \"Unknown\"),\n",
    "        },\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6776152",
   "metadata": {},
   "outputs": [],
   "source": [
    "contents = df.apply(\n",
    "    create_document, axis=1\n",
    ").tolist()  # apply manipulates the columns default."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff98773",
   "metadata": {},
   "source": [
    "## âœ‚ï¸ Step 03 â€” Text Chunking\n",
    "\n",
    "Test chunking strategies for longer documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19f87e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    "    length_function=len,\n",
    "    separators=[\n",
    "        \"\\n\\n\",\n",
    "        \"\\n\",\n",
    "        \" \",\n",
    "        \"\",\n",
    "    ],  # Prefer to separate at paragraph, new line, and space\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "349aed80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¬ Original document:\n",
      "Name: å®äºé£Ÿå“å·§å…‹åŠ›è§€å…‰å·¥å» \n",
      "Region: æ¡ƒåœ’å¸‚\n",
      "Address: æ¡ƒåœ’ç¸£å…«å¾·å¸‚å»ºåœ‹è·¯386è™Ÿ\n",
      "Description: å·§å…‹åŠ›å…±å’Œåœ‹æ˜¯ä¸€åº§ä»¥å·§å…‹åŠ›ç‚ºä¸»é¡Œçš„è§€å…‰å·¥å» ï¼Œå»ºç¯‰è¨­è¨ˆã€é¤¨å…§ä¸»é¡Œè¨­è¨ˆçš†ä»¥å·§å…‹åŠ›ç‚ºä¸»é¡Œï¼Œé€™è£¡ä¹Ÿæä¾›è±å¯Œçš„å·§å…‹åŠ›ç›¸é—œçŸ¥è­˜ï¼Œäº¦å¯ä»¥DIYå‰µä½œå·§å…‹åŠ›ï¼Œç‚ºä¸€å¯“æ•™æ–¼æ¨‚ã€é©åˆè¦ªå­ä¼‘é–’å¨›æ¨‚çš„çµ•ä½³å»è™•ã€‚\n",
      "Ticket: æ”¶è²»æ–¹å¼è«‹é›»æ´½\n",
      "ğŸ“Š Length: 173 characters\n"
     ]
    }
   ],
   "source": [
    "sample_doc = contents[0]\n",
    "info(\"Original document:\")\n",
    "print(sample_doc[\"content\"])\n",
    "data(f\"Length: {len(sample_doc['content'])} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2d77c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Separate to 1 chunks\n",
      "\n",
      "--- Chunk 1 (173 characters) ---\n",
      "Name: å®äºé£Ÿå“å·§å…‹åŠ›è§€å…‰å·¥å» \n",
      "Region: æ¡ƒåœ’å¸‚\n",
      "Address: æ¡ƒåœ’ç¸£å…«å¾·å¸‚å»ºåœ‹è·¯386è™Ÿ\n",
      "Description: å·§å…‹åŠ›å…±å’Œåœ‹æ˜¯ä¸€åº§ä»¥å·§å…‹åŠ›ç‚ºä¸»é¡Œçš„è§€å…‰å·¥å» ï¼Œå»ºç¯‰è¨­è¨ˆã€é¤¨å…§ä¸»é¡Œè¨­è¨ˆçš†ä»¥å·§å…‹åŠ›ç‚ºä¸»é¡Œï¼Œé€™è£¡ä¹Ÿæä¾›è±å¯Œçš„å·§å…‹åŠ›ç›¸é—œçŸ¥è­˜ï¼Œäº¦å¯ä»¥DIYå‰µä½œå·§å…‹åŠ›ï¼Œç‚ºä¸€å¯“æ•™æ–¼æ¨‚ã€é©åˆè¦ªå­ä¼‘é–’å¨›æ¨‚çš„çµ•ä½³å»è™•ã€‚\n",
      "Ticket: æ”¶è²»æ–¹å¼è«‹é›»æ´½\n"
     ]
    }
   ],
   "source": [
    "# Chunk\n",
    "chunks = text_splitter.split_text(sample_doc[\"content\"])\n",
    "print(f\"Separate to {len(chunks)} chunks\")\n",
    "\n",
    "for i, chunk in enumerate(chunks, 1):\n",
    "    print(f\"\\n--- Chunk {i} ({len(chunk)} characters) ---\")\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4cae01fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docs length summary:\n",
      "            length\n",
      "count  5086.000000\n",
      "mean    204.130751\n",
      "std     135.558116\n",
      "min      21.000000\n",
      "25%      85.000000\n",
      "50%     175.000000\n",
      "75%     287.750000\n",
      "max     643.000000\n"
     ]
    }
   ],
   "source": [
    "doc_lengths = [len(doc[\"content\"]) for doc in contents]\n",
    "length_df = pd.DataFrame({\n",
    "    \"length\": doc_lengths\n",
    "})\n",
    "\n",
    "print(\"Docs length summary:\")\n",
    "print(length_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e9a8e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Documents over 500 characters: 99 / 5086\n",
      "ğŸ’¬ Ratio: 1.9%\n"
     ]
    }
   ],
   "source": [
    "need_chunking = sum(1 for length in doc_lengths if length > 500)\n",
    "data(f\"Documents over 500 characters: {need_chunking} / {len(contents)}\")\n",
    "info(f\"Ratio: {need_chunking / len(contents) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9f69d2",
   "metadata": {},
   "source": [
    "## ğŸ’¾ Step 04 â€” Save Processed Data\n",
    "\n",
    "Export processed documents to JSON format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "44ae81fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ Saved 5086 documents to: c:\\Users\\dinni\\OneDrive\\æ¡Œé¢\\Travel_rag\\data\\processed\\documents.json\n",
      "ğŸ“Š File size: 3.21 MB\n"
     ]
    }
   ],
   "source": [
    "PROCESSED_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "output_path = PROCESSED_DATA_DIR / \"documents.json\"\n",
    "\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(contents, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "save(f\"Saved {len(contents)} documents to: {output_path}\")\n",
    "data(f\"File size: {output_path.stat().st_size / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c4a8cece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Verification complete!\n",
      "ğŸ“Š Original documents: 5086\n",
      "ğŸ“Š Loaded documents: 5086\n",
      "âœ… Data consistency: True\n",
      "ğŸ Chapter 02 completed!\n"
     ]
    }
   ],
   "source": [
    "# Double check\n",
    "with open(output_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    loaded_docs = json.load(f)\n",
    "\n",
    "success(\"Verification complete!\")\n",
    "data(f\"Original documents: {len(contents)}\")\n",
    "data(f\"Loaded documents: {len(loaded_docs)}\")\n",
    "success(f\"Data consistency: {len(contents) == len(loaded_docs)}\")\n",
    "done(\"Chapter 02 completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "travel-rag-k8jaxz6c-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
