{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95d95af7",
   "metadata": {},
   "source": [
    "# üìñ Chapter 02 ‚Äî Data Processing & Chunking\n",
    "\n",
    "## üéØ Objectives\n",
    "\n",
    "In this chapter, we will process the raw tourism data into a format suitable for RAG implementation.\n",
    "\n",
    "We will create unified document structures, implement chunking strategies, and save processed data for vector database ingestion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1fae3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from src.config import RAW_DATA_DIR, PROCESSED_DATA_DIR\n",
    "import json\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c847f36d",
   "metadata": {},
   "source": [
    "## üßπ Step 01 ‚Äî Data Loading\n",
    "\n",
    "Load the raw tourism data from JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57c990c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(RAW_DATA_DIR / \"scenic_spot.json\", \"r\", encoding=\"utf-8-sig\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "attraction = data[\"XML_Head\"][\"Infos\"][\"Info\"]\n",
    "df = pd.DataFrame(attraction)\n",
    "\n",
    "text_cols = [\"Add\", \"Description\", \"Opentime\", \"Ticketinfo\", \"Travellinginfo\"]\n",
    "df[text_cols] = df[text_cols].fillna(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9dd86a5",
   "metadata": {},
   "source": [
    "## üìÑ Step 02 ‚Äî Document Creation\n",
    "\n",
    "Transform each attraction into a unified document format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "705dcaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_document(row):\n",
    "    # Combine the text columns\n",
    "    content_parts = [\n",
    "        f\"Name: {row['Name']}\",\n",
    "        f\"Region: {row['Region']}\",\n",
    "    ]\n",
    "\n",
    "    # if column exists, return it, otherwise, return \"\", and \"\" is false in if\n",
    "    if str(row.get(\"Add\", \"\")).strip():\n",
    "        content_parts.append(f\"Address: {row['Add']}\")\n",
    "\n",
    "    if str(row.get(\"Description\", \"\")).strip():\n",
    "        content_parts.append(f\"Description: {row['Description']}\")\n",
    "\n",
    "    if str(row.get(\"Opentime\", \"\")).strip():\n",
    "        content_parts.append(f\"Open_time: {row['Opentime']}\")\n",
    "\n",
    "    if str(row.get(\"Ticketinfo\", \"\")).strip():\n",
    "        content_parts.append(f\"Ticket: {row['Ticketinfo']}\")\n",
    "\n",
    "    # Filter the empty\n",
    "    content = \"\\n\".join([part for part in content_parts if part])\n",
    "\n",
    "    return {\n",
    "        \"id\": row[\"Id\"],\n",
    "        \"content\": content,\n",
    "        \"metadata\": {\n",
    "            \"name\": row[\"Name\"],\n",
    "            \"region\": row[\"Region\"],\n",
    "            \"category\": row.get(\"Class1\", \"Unknown\"),\n",
    "        },\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6776152",
   "metadata": {},
   "outputs": [],
   "source": [
    "contents = df.apply(\n",
    "    create_document, axis=1\n",
    ").tolist()  # apply manipulates the columns default."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff98773",
   "metadata": {},
   "source": [
    "## ‚úÇÔ∏è Step 03 ‚Äî Text Chunking\n",
    "\n",
    "Test chunking strategies for longer documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19f87e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    "    length_function=len,\n",
    "    separators=[\n",
    "        \"\\n\\n\",\n",
    "        \"\\n\",\n",
    "        \" \",\n",
    "        \"\",\n",
    "    ],  # Prefer to separate at paragraph, new line, and space\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "349aed80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original docs:\n",
      "Name: ÂÆè‰∫ûÈ£üÂìÅÂ∑ßÂÖãÂäõËßÄÂÖâÂ∑•Âª†\n",
      "Region: Ê°ÉÂúíÂ∏Ç\n",
      "Address: Ê°ÉÂúíÁ∏£ÂÖ´Âæ∑Â∏ÇÂª∫ÂúãË∑Ø386Ëôü\n",
      "Description: Â∑ßÂÖãÂäõÂÖ±ÂíåÂúãÊòØ‰∏ÄÂ∫ß‰ª•Â∑ßÂÖãÂäõÁÇ∫‰∏ªÈ°åÁöÑËßÄÂÖâÂ∑•Âª†ÔºåÂª∫ÁØâË®≠Ë®à„ÄÅÈ§®ÂÖß‰∏ªÈ°åË®≠Ë®àÁöÜ‰ª•Â∑ßÂÖãÂäõÁÇ∫‰∏ªÈ°åÔºåÈÄôË£°‰πüÊèê‰æõË±êÂØåÁöÑÂ∑ßÂÖãÂäõÁõ∏ÈóúÁü•Ë≠òÔºå‰∫¶ÂèØ‰ª•DIYÂâµ‰ΩúÂ∑ßÂÖãÂäõÔºåÁÇ∫‰∏ÄÂØìÊïôÊñºÊ®Ç„ÄÅÈÅ©ÂêàË¶™Â≠ê‰ºëÈñíÂ®õÊ®ÇÁöÑÁµï‰Ω≥ÂéªËôï„ÄÇ\n",
      "Ticket: Êî∂Ë≤ªÊñπÂºèË´ãÈõªÊ¥Ω\n",
      "\n",
      "length: 173 characters\n"
     ]
    }
   ],
   "source": [
    "sample_doc = contents[0]\n",
    "print(\"Original docs:\")\n",
    "print(sample_doc[\"content\"])\n",
    "print(f\"\\nlength: {len(sample_doc[\"content\"])} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2d77c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Separate to 1 chunks\n",
      "\n",
      "--- Chunk 1 (173 characters) ---\n",
      "Name: ÂÆè‰∫ûÈ£üÂìÅÂ∑ßÂÖãÂäõËßÄÂÖâÂ∑•Âª†\n",
      "Region: Ê°ÉÂúíÂ∏Ç\n",
      "Address: Ê°ÉÂúíÁ∏£ÂÖ´Âæ∑Â∏ÇÂª∫ÂúãË∑Ø386Ëôü\n",
      "Description: Â∑ßÂÖãÂäõÂÖ±ÂíåÂúãÊòØ‰∏ÄÂ∫ß‰ª•Â∑ßÂÖãÂäõÁÇ∫‰∏ªÈ°åÁöÑËßÄÂÖâÂ∑•Âª†ÔºåÂª∫ÁØâË®≠Ë®à„ÄÅÈ§®ÂÖß‰∏ªÈ°åË®≠Ë®àÁöÜ‰ª•Â∑ßÂÖãÂäõÁÇ∫‰∏ªÈ°åÔºåÈÄôË£°‰πüÊèê‰æõË±êÂØåÁöÑÂ∑ßÂÖãÂäõÁõ∏ÈóúÁü•Ë≠òÔºå‰∫¶ÂèØ‰ª•DIYÂâµ‰ΩúÂ∑ßÂÖãÂäõÔºåÁÇ∫‰∏ÄÂØìÊïôÊñºÊ®Ç„ÄÅÈÅ©ÂêàË¶™Â≠ê‰ºëÈñíÂ®õÊ®ÇÁöÑÁµï‰Ω≥ÂéªËôï„ÄÇ\n",
      "Ticket: Êî∂Ë≤ªÊñπÂºèË´ãÈõªÊ¥Ω\n"
     ]
    }
   ],
   "source": [
    "# Chunk\n",
    "chunks = text_splitter.split_text(sample_doc[\"content\"])\n",
    "print(f\"Separate to {len(chunks)} chunks\")\n",
    "\n",
    "for i, chunk in enumerate(chunks, 1):\n",
    "    print(f\"\\n--- Chunk {i} ({len(chunk)} characters) ---\")\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4cae01fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docs length summary:\n",
      "            length\n",
      "count  5086.000000\n",
      "mean    204.130751\n",
      "std     135.558116\n",
      "min      21.000000\n",
      "25%      85.000000\n",
      "50%     175.000000\n",
      "75%     287.750000\n",
      "max     643.000000\n"
     ]
    }
   ],
   "source": [
    "doc_lengths = [len(doc[\"content\"]) for doc in contents]\n",
    "length_df = pd.DataFrame({\n",
    "    \"length\": doc_lengths\n",
    "})\n",
    "\n",
    "print(\"Docs length summary:\")\n",
    "print(length_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e9a8e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The docs with over 500 characters: 99 / 5086\n",
      "Ratio: 1.9%\n"
     ]
    }
   ],
   "source": [
    "need_chunking = sum(1 for length in doc_lengths if length > 500)\n",
    "print(f\"The docs with over 500 characters: {need_chunking} / {len(contents)}\")\n",
    "print(f\"Ratio: {need_chunking / len(contents) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9f69d2",
   "metadata": {},
   "source": [
    "## üíæ Step 04 ‚Äî Save Processed Data\n",
    "\n",
    "Export processed documents to JSON format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "44ae81fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 5086 docs to:\n",
      "c:\\Users\\dinni\\OneDrive\\Ê°åÈù¢\\Travel_rag\\data\\processed\\documents.json ‚úÖ \n",
      "File capacity: 3.21 MB\n"
     ]
    }
   ],
   "source": [
    "PROCESSED_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "output_path = PROCESSED_DATA_DIR / \"documents.json\"\n",
    "\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(contents, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Saved {len(contents)} docs to:\")\n",
    "print(f\"{output_path} ‚úÖ \")\n",
    "print(f\"File capacity: {output_path.stat().st_size / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c4a8cece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! ‚úÖ \n",
      "Original docs: 5086\n",
      "Loaded docs: 5086\n",
      "Data consistency: True\n"
     ]
    }
   ],
   "source": [
    "# Double check\n",
    "with open(output_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    loaded_docs = json.load(f)\n",
    "\n",
    "print(\"Success! ‚úÖ \")\n",
    "print(f\"Original docs: {len(contents)}\")\n",
    "print(f\"Loaded docs: {len(loaded_docs)}\")\n",
    "print(f\"Data consistency: {len(contents) == len(loaded_docs)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "travel-rag-k8jaxz6c-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
